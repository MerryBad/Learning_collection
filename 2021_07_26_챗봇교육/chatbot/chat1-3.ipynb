{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcb0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## intent 훈련과 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb98466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5665e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105658, 15)\n",
      "105658\n",
      "Epoch 1/2\n",
      "3698/3698 [==============================] - 68s 18ms/step - loss: 0.0494 - accuracy: 0.9845 - val_loss: 0.0127 - val_accuracy: 0.9951\n",
      "Epoch 2/2\n",
      "3698/3698 [==============================] - 71s 19ms/step - loss: 0.0158 - accuracy: 0.9946 - val_loss: 0.0101 - val_accuracy: 0.9938\n",
      "529/529 [==============================] - 1s 1ms/step - loss: 0.0099 - accuracy: 0.9939\n",
      "Accuracy: 99.394226\n",
      "loss: 0.009903\n"
     ]
    }
   ],
   "source": [
    "## models/intent/train_model.py\n",
    "# 필요한 모듈 임포트\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "\n",
    "# 데이터 읽어오기 : 1\n",
    "train_file = \"./models/intent/total_train_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()\n",
    "\n",
    "from utils.Preprocess import Preprocess\n",
    "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dict.bin',\n",
    "               userdic='./utils/user_dic.tsv')\n",
    "\n",
    "# 단어 시퀀스 생성\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag=True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)\n",
    "\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터 : 2\n",
    "# 단어 시퀀스 벡터 크기\n",
    "from config.GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "# (105658, 15)\n",
    "print(padded_seqs.shape)\n",
    "print(len(intents)) #105658\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성 : 3\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.word_index) + 1 #전체 단어 개수\n",
    "\n",
    "\n",
    "# CNN 모델 정의 : 4\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=4,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=5,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# 3,4,5gram 이후 합치기\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(5, name='logits')(dropout_hidden)\n",
    "predictions = Dense(5, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "\n",
    "# 모델 생성 : 5\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 모델 학습 : 6\n",
    "# model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=2, verbose=1)\n",
    "\n",
    "\n",
    "# 모델 평가(테스트 데이터 셋 이용) : 7\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))\n",
    "print('loss: %f' % (loss))\n",
    "\n",
    "\n",
    "# 모델 저장 : 8\n",
    "model.save('./models/intent/intent_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dcdad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 테스트\n",
    "## models/intent/IntentModel.py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "\n",
    "# 의도 분류 모델 모듈\n",
    "class IntentModel:\n",
    "    def __init__(self, model_name, proprocess):\n",
    "\n",
    "        # 의도 클래스 별 레이블\n",
    "        self.labels = {0: \"인사\", 1: \"욕설\", 2: \"주문\", 3: \"예약\", 4: \"기타\"}\n",
    "\n",
    "        # 의도 분류 모델 불러오기\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        # 챗봇 Preprocess 객체\n",
    "        self.p = proprocess\n",
    "\n",
    "\n",
    "    # 의도 클래스 예측\n",
    "    def predict_class(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 단어 시퀀스 벡터 크기\n",
    "        from config.GlobalParams import MAX_SEQ_LEN\n",
    "\n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "        return predict_class.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63a228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 탕수육 주문 가능한가요?\n",
      "의도 예측 점수 :  [[3.3155530e-21 2.0667667e-17 1.0000000e+00 1.0614781e-14 5.1284097e-16]]\n",
      "의도 예측 클래스 :  [2]\n",
      "의도  :  주문\n"
     ]
    }
   ],
   "source": [
    "## test/model_intent_test.py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "intent_labels = {0: \"인사\", 1: \"욕설\", 2: \"주문\", 3: \"예약\", 4: \"기타\"}\n",
    "\n",
    "# 의도 분류 모델 불러오기\n",
    "model = load_model('./models/intent/intent_model.h5')\n",
    "\n",
    "query = \"오늘 탕수육 주문 가능한가요?\"\n",
    "\n",
    "from utils.Preprocess import Preprocess\n",
    "\n",
    "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dict.bin',\n",
    "               userdic='./utils/user_dic.tsv')\n",
    "pos = p.pos(query)\n",
    "keywords = p.get_keywords(pos, without_tag=True)\n",
    "seq = p.get_wordidx_sequence(keywords)\n",
    "sequences = [seq]\n",
    "\n",
    "# 단어 시퀀스 벡터 크기\n",
    "from config.GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "predict = model.predict(padded_seqs)\n",
    "predict_class = tf.math.argmax(predict, axis=1)\n",
    "print(query)\n",
    "print(\"의도 예측 점수 : \", predict)\n",
    "print(\"의도 예측 클래스 : \", predict_class.numpy())\n",
    "print(\"의도  : \", intent_labels[predict_class.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42662ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
