{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15fc162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 챗봇 엔진 NER 모델과 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9269264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c963bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 크기 : \n",
      " 61999\n",
      "0번 째 샘플 단어 시퀀스 : \n",
      " ['가락지빵', '주문', '하', '고', '싶', '어요']\n",
      "0번 째 샘플 bio 태그 : \n",
      " ['B_FOOD', 'O', 'O', 'O', 'O', 'O']\n",
      "샘플 단어 시퀀스 최대 길이 : 168\n",
      "샘플 단어 시퀀스 평균 길이 : 8.796238649010467\n",
      "BIO 태그 사전 크기 : 10\n",
      "단어 사전 크기 : 17869\n",
      "학습 샘플 시퀀스 형상 :  (49599, 40)\n",
      "학습 샘플 레이블 형상 :  (49599, 40, 10)\n",
      "테스트 샘플 시퀀스 형상 :  (12400, 40)\n",
      "테스트 샘플 레이블 형상 :  (12400, 40, 10)\n",
      "Epoch 1/2\n",
      "388/388 [==============================] - 117s 303ms/step - loss: 0.0251 - accuracy: 0.9682\n",
      "Epoch 2/2\n",
      "388/388 [==============================] - 161s 416ms/step - loss: 0.0083 - accuracy: 0.9876\n",
      "388/388 [==============================] - 19s 48ms/step - loss: 0.0099 - accuracy: 0.9857\n",
      "평가 결과 :  0.9856594204902649\n"
     ]
    }
   ],
   "source": [
    "## models/ner/train_model.py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from utils.Preprocess import Preprocess\n",
    "\n",
    "# 학습 파일 불러오기\n",
    "def read_file(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents\n",
    "\n",
    "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dict.bin',\n",
    "               userdic='./utils/user_dic.tsv')\n",
    "\n",
    "# 학습용 말뭉치 데이터를 불러옴\n",
    "corpus = read_file('./models/ner/ner_train.txt')\n",
    "\n",
    "# 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
    "sentences, tags = [], []\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    for w in t:\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "    \n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "\n",
    "\n",
    "print(\"샘플 크기 : \\n\", len(sentences))\n",
    "print(\"0번 째 샘플 단어 시퀀스 : \\n\", sentences[0])\n",
    "print(\"0번 째 샘플 bio 태그 : \\n\", tags[0])\n",
    "print(\"샘플 단어 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
    "print(\"샘플 단어 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))\n",
    "\n",
    "# 토크나이저 정의\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False 소문자로 변환하지 않는다.\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "# 단어사전 및 태그 사전 크기\n",
    "vocab_size = len(p.word_index) + 1\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
    "print(\"단어 사전 크기 :\", vocab_size)\n",
    "\n",
    "# 학습용 단어 시퀀스 생성\n",
    "x_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환 하기 위해 사용\n",
    "index_to_ner[0] = 'PAD'\n",
    "\n",
    "# 시퀀스 패딩 처리\n",
    "max_len = 40\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 8:2의 비율로 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
    "                                                    test_size=.2,\n",
    "                                                    random_state=1234)\n",
    "\n",
    "# 출력 데이터를 one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
    "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
    "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
    "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
    "\n",
    "\n",
    "# 모델 정의 (Bi-LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=2)  # 10번은 너무 오래 결려서 2번만 학습\n",
    "\n",
    "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
    "model.save('./models/ner/ner_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29309b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NP       1.00      1.00      1.00       303\n",
      "           _       0.57      0.48      0.52       658\n",
      "         _DT       1.00      1.00      1.00     13683\n",
      "       _FOOD       1.00      1.00      1.00     11655\n",
      "         _LC       0.69      0.63      0.66       314\n",
      "         _OG       0.66      0.37      0.47       460\n",
      "         _PS       0.45      0.66      0.53       396\n",
      "         _TI       0.82      0.51      0.63        61\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     27530\n",
      "   macro avg       0.77      0.70      0.73     27530\n",
      "weighted avg       0.97      0.96      0.97     27530\n",
      "\n",
      "F1-score: 96.8%\n"
     ]
    }
   ],
   "source": [
    "# 시퀀스를 NER 태그로 변환\n",
    "def sequences_to_tag(sequences):  # 예측값을 index_to_ner를 사용하여 태깅 정보로 변경하는 함수.\n",
    "    result = []\n",
    "    for sequence in sequences:  # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
    "        temp = []\n",
    "        for pred in sequence:  # 시퀀스로부터 예측값을 하나씩 꺼낸다.\n",
    "            pred_index = np.argmax(pred)  # 예를 들어 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
    "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))  # 'PAD'는 'O'로 변경\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "\n",
    "# f1 스코어 계산을 위해 사용\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "# 테스트 데이터셋의 NER 예측\n",
    "y_predicted = model.predict(x_test)\n",
    "pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
    "test_tags = sequences_to_tag(y_test)    # 실제 NER\n",
    "\n",
    "# F1 평가 결과\n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df84099",
   "metadata": {},
   "outputs": [],
   "source": [
    "## models/ner/NerModel.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "\n",
    "# 개체명 인식 모델 모듈\n",
    "class NerModel:\n",
    "    def __init__(self, model_name, proprocess):\n",
    "\n",
    "        # BIO 태그 클래스 별 레이블\n",
    "        self.index_to_ner = {1:'O',2:'B_DT',3:'B_FOOD',4:'I',5:'B_OG',6:'B_PS',7:'B_LC',8:'NNP',9:'B_TI',0:'PAD'}\n",
    "\n",
    "        # 의도 분류 모델 불러오기\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        # 챗봇 Preprocess 객체\n",
    "        self.p = proprocess\n",
    "\n",
    "\n",
    "    # 개체명 클래스 예측\n",
    "    def predict(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)\n",
    "\n",
    "        tags = [self.index_to_ner[i] for i in predict_class.numpy()[0]]\n",
    "        return list(zip(keywords, tags))\n",
    "\n",
    "    def predict_tags(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)\n",
    "\n",
    "        tags = []\n",
    "        for tag_idx in predict_class.numpy()[0]:\n",
    "            if tag_idx == 1: continue\n",
    "            tags.append(self.index_to_ner[tag_idx])\n",
    "\n",
    "        if len(tags) == 0: return None\n",
    "        return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b93e0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'B_DT'), ('오후', 'B_DT'), ('13시', 'B_DT'), ('2분', 'B_DT'), ('탕수육', 'B_FOOD'), ('주문', 'O'), ('하', 'O'), ('싶', 'O')]\n",
      "['B_DT', 'B_DT', 'B_DT', 'B_DT', 'B_FOOD']\n"
     ]
    }
   ],
   "source": [
    "## test/model_ner_test.py\n",
    "from utils.Preprocess import Preprocess\n",
    "from models.ner.NerModel import NerModel\n",
    "\n",
    "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dict.bin',\n",
    "               userdic='./utils/user_dic.tsv')\n",
    "\n",
    "\n",
    "ner = NerModel(model_name='./models/ner/ner_model.h5', proprocess=p)\n",
    "query = '오늘 오후 13시 2분에 탕수육 주문 하고 싶어요'\n",
    "predicts = ner.predict(query)\n",
    "tags = ner.predict_tags(query)\n",
    "print(predicts)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b67dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
